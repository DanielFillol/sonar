package main

import (
	"bufio"
	"fmt"
	"log"
	"os"
	"sonar/app"
	"time"
)

var (
	// The original user prompt.
	prompt = ""
)

func main() {
	// Check if the prompt is empty; if so, collect the prompt from the terminal.
	if prompt == "" {
		fmt.Print("Pergunte ao which: ")
		scanner := bufio.NewScanner(os.Stdin)
		if scanner.Scan() {
			prompt = scanner.Text()
		}
	}
	start := time.Now()

	// llm pode ser "deepseek", "gpt-mini" ou "gpt-full"
	err := app.GetPromptResponse("deepseek", prompt)
	if err != nil {
		log.Fatal(err)
	}

	fmt.Println(time.Since(start))
}

package app

import (
	"errors"
	"fmt"
	"github.com/joho/godotenv"
	"log"
	"os"
	"sonar/app/deepseek"
	"sonar/app/gpt"
	"sonar/app/juit"
	"sonar/app/perplexity"
)

var (
	gptRelevantCaseLaw string

	gptSimplePrompt string

	gptClassifier string

	gptAuthors string

	perplexitySearcher string

	perplexityLaw string

	gptSpecialist string
)

func populateVars() error {
	err := godotenv.Load(".env")
	if err != nil {
		return err
	}

	// Retrieve the prompts from the environment.
	gptRelevantCaseLaw = os.Getenv("gptRelevantCaseLaw")
	gptSimplePrompt = os.Getenv("gptSimplePrompt")
	gptClassifier = os.Getenv("gptClassifier")
	gptAuthors = os.Getenv("gptAuthors")
	perplexitySearcher = os.Getenv("perplexitySearcher")
	perplexityLaw = os.Getenv("perplexityLaw")
	gptSpecialist = os.Getenv("gptSpecialist")

	return nil
}

func createFile(content string) error {
	file, err := os.Create("resposta.md")
	if err != nil {
		return errors.New("Erro ao criar o arquivo:" + err.Error())
	}

	defer file.Close()

	_, err = file.WriteString(content)
	if err != nil {
		return errors.New("Erro ao escrever o arquivo:" + err.Error())
	}

	return nil
}

func finalAnswer(llm, system, field, authors, quotes, linkQuotes, laws, linkLaws, prompt, juris string) (string, error) {
	log.Println("Gerando texto final...")
	specialistInput := "O ramo do direito é:\n" + field +
		"\nOs doutrinadores relevantes são:\n" + authors +
		"\nA doutrina relevante é:\n" + quotes +
		"\nOs links relevantes são:\n" + linkQuotes +
		"\nAs leis relevantes são:\n" + laws +
		"\nOs links legislativos relevantes são relevantes são:\n" + linkLaws +
		"\nO prompt original do usuário é:\n" + prompt +
		"\nAs Jurisprudências retornadas são:\n" + juris

	if llm == "deepseek" {
		specialist, err := deepseek.Search(system, specialistInput, "deepseek-reasoner")
		if err != nil {

			fmt.Println(specialistInput)

			return "", errors.New("Erro ao processar a resposta final:" + err.Error())
		}

		return specialist.Choices[0].Message.Content, nil
	} else if llm == "gpt-mini" {
		specialist, err := gpt.Search(system, specialistInput, "gpt-4o-mini")
		if err != nil {
			return "", errors.New("Erro ao processar a resposta final:" + err.Error())
		}

		return specialist.Choices[0].Message.Content, nil
	} else if llm == "gpt-full" {
		specialist, err := gpt.Search(system, specialistInput, "chatgpt-4o-latest")
		if err != nil {
			return "", errors.New("Erro ao processar a resposta final:" + err.Error())
		}

		return specialist.Choices[0].Message.Content, nil
	} else {
		return "", errors.New("Erro ao processar a resposta final:" + llm)
	}

}

// GetPromptResponse llm must be:
//   - deepseek
//   - gpt-mini
//   - gpt-full
func GetPromptResponse(llm, prompt string) error {
	err := populateVars()
	if err != nil {
		return err
	}

	// Verifies if the prompt is eligible for case law search
	relevant, err := juit.ShouldCallJurisprudencia(gptRelevantCaseLaw, prompt)
	if err != nil {
		return err
	}

	// Get Relevant Case Law
	var promptJuris *string
	var juris *string
	if relevant {
		promptJuris, err = juit.CreateQueryForJurisprudencia(gptSimplePrompt, prompt)
		if err != nil {
			return err
		}

		juris, err = juit.CallAPIjurisprudencia(*promptJuris)
		if err != nil {
			return err
		}
	}

	// Classify the legal field of the prompt.
	field, err := gpt.ClassifyLawField(gptClassifier, prompt)
	if err != nil {
		return err
	}

	// Get the main doctrinal experts for the classified legal field.
	authors, err := gpt.GetRelevantAuthors(gptAuthors, *field)
	if err != nil {
		return err
	}

	// Search for relevant citations from the doctrinal experts.
	if authors == nil || promptJuris == nil {
		return errors.New("autores não foram localizados ou prompt para API de jurisprudência não foi localizado")
	}

	doctrines, err := perplexity.SearchForQuotes(perplexitySearcher, *field, *authors, *promptJuris, prompt)
	if err != nil {
		return err
	}

	// Search for relevant laws in official sites
	laws, err := perplexity.SearchForLaws(perplexityLaw, *field, prompt, *promptJuris)
	if err != nil {
		return err
	}

	// Structure the final answer by integrating all the gathered information.
	answer, err := finalAnswer(llm, gptSpecialist, *field, *authors, doctrines.Response, doctrines.Links, laws.Response, laws.Links, prompt, *juris)
	if err != nil {
		return err
	}

	err = createFile(answer)
	if err != nil {
		return err
	}

	log.Println("A resposta foi salva no arquivo resposta.md")
	return nil
}

package perplexity

import "log"

type Perplexity struct {
	Response string
	Links    string
}

// RequestPerplexity represents a request payload for the Perplexity chat completions API.
type RequestPerplexity struct {
	// Model specifies the name of the model to use for generating the completion.
	// Refer to the supported models in the Perplexity API documentation.
	Model string `json:"model"`

	// Messages is a list of messages that make up the conversation so far.
	// Each message should have a role (system, user, or assistant) and content.
	Messages []Message `json:"messages"`

	// MaxTokens defines the maximum number of tokens to generate in the completion.
	// The total of prompt tokens and max_tokens must not exceed the model's context window.
	// If not specified, the model will generate tokens until it reaches a stop sequence or the context limit.
	MaxTokens int `json:"max_tokens,omitempty"`

	// Temperature controls the randomness of the output.
	// Values range from 0 to 2, with higher values producing more random outputs.
	// Default is 0.2.
	Temperature float64 `json:"temperature,omitempty"`

	// TopP implements nucleus sampling, considering only the tokens with top_p probability mass.
	// Values range from 0 to 1. Default is 0.9.
	TopP float64 `json:"top_p,omitempty"`

	// SearchDomainFilter limits the citations used by the online model to URLs from the specified domains.
	// Up to 3 domains can be specified. Prefix a domain with '-' to blacklist it.
	// Only available in certain tiers.
	SearchDomainFilter []string `json:"search_domain_filter,omitempty"`

	// ReturnImages determines whether the response should include images.
	// Default is false. Only available in certain tiers.
	ReturnImages bool `json:"return_images,omitempty"`

	// ReturnRelatedQuestions determines whether the response should include related questions.
	// Default is false. Only available in certain tiers.
	ReturnRelatedQuestions bool `json:"return_related_questions,omitempty"`

	// SearchRecencyFilter restricts search results to a specified time frame.
	// Acceptable values are "hour", "day", "week", or "month".
	SearchRecencyFilter string `json:"search_recency_filter,omitempty"`

	// TopK specifies the number of highest probability tokens to consider for top-k filtering.
	// Values range from 0 to 2048. A value of 0 disables top-k filtering. Default is 0.
	TopK int `json:"top_k,omitempty"`

	// Stream determines whether to stream the response incrementally using server-sent events.
	// Default is false.
	Stream bool `json:"stream,omitempty"`

	// PresencePenalty applies a penalty to the likelihood of new tokens based on their presence in the text so far.
	// Values range from -2.0 to 2.0. Positive values increase the model's likelihood to discuss new topics.
	// Incompatible with FrequencyPenalty. Default is 0.
	PresencePenalty float64 `json:"presence_penalty,omitempty"`

	// FrequencyPenalty applies a penalty to the likelihood of new tokens based on their frequency in the text so far.
	// Values greater than 1.0 decrease the model's tendency to repeat the same line verbatim.
	// Incompatible with PresencePenalty. Default is 1.
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"`

	// ResponseFormat enables structured outputs with a JSON or Regex schema.
	// Only available in certain tiers.
	ResponseFormat interface{} `json:"response_format,omitempty"`
}

// Message represents a single message in the conversation.
type Message struct {
	// Role specifies the role of the message author.
	// Acceptable values are "system", "user", or "assistant".
	Role string `json:"role"`

	// Content contains the text content of the message.
	Content string `json:"content"`
}

func NewPerplexityPayload() *RequestPerplexity {
	return &RequestPerplexity{
		Model: "sonar-pro",
		//MaxTokens:              123,
		Temperature:            0.2,
		TopP:                   0.9,
		SearchDomainFilter:     nil,
		ReturnImages:           false,
		ReturnRelatedQuestions: false,
		SearchRecencyFilter:    "",
		TopK:                   0,
		Stream:                 false,
		PresencePenalty:        0,
		FrequencyPenalty:       1,
		ResponseFormat:         nil,
	}
}

func (r *RequestPerplexity) NewMessage(message Message) {
	r.Messages = append(r.Messages, message)
}

// AddSearchFilter
//
//	Given a list of domains, limit the citations used by the online model to URLs from the specified domains.
//	Currently limited to only 3 domains for whitelisting and blacklisting.
//	For blacklisting add a - to the beginning of the domain string.
func (r *RequestPerplexity) AddSearchFilter(urls []string) {
	for _, url := range urls {
		r.SearchDomainFilter = append(r.SearchDomainFilter, url)
	}
}

// ResponsePerplexity represents the response payload from the Perplexity chat completions API.
type ResponsePerplexity struct {
	// Id is a unique identifier generated for each response.
	Id string `json:"id"`

	// Model specifies the name of the model used to generate the response.
	Model string `json:"model"`

	// Created is the Unix timestamp (in seconds) indicating when the completion was created.
	Created int `json:"created"`

	// Usage provides statistics about token usage for the completion request.
	Usage struct {
		// PromptTokens is the number of tokens provided in the request prompt.
		PromptTokens int `json:"prompt_tokens"`

		// CompletionTokens is the number of tokens generated in the response.
		CompletionTokens int `json:"completion_tokens"`

		// TotalTokens is the total number of tokens used (prompt + completion).
		TotalTokens int `json:"total_tokens"`
	} `json:"usage"`

	// Citations contains a list of URLs cited in the generated answer.
	Citations []string `json:"citations"`

	// Object specifies the object type, which is always "chat.completion" for this endpoint.
	Object string `json:"object"`

	// Choices is a list of completion choices generated for the input prompt.
	Choices []struct {
		// Index is the position of this completion choice in the list.
		Index int `json:"index"`

		// FinishReason indicates why the model stopped generating tokens. Possible values include:
		// - "stop": The model reached a natural stopping point.
		// - "length": The maximum number of tokens specified in the request was reached.
		FinishReason string `json:"finish_reason"`

		// Message contains the message generated by the model.
		Message struct {
			// Role specifies the role of the message author. Possible values are:
			// - "system"
			// - "user"
			// - "assistant"
			Role string `json:"role"`

			// Content is the text content of the message.
			Content string `json:"content"`
		} `json:"message"`

		// Delta contains the incrementally streamed next tokens. This field is only meaningful when streaming is enabled (`stream=true`).
		Delta struct {
			// Role specifies the role of the message author. Possible values are:
			// - "system"
			// - "user"
			// - "assistant"
			Role string `json:"role"`

			// Content is the text content of the message.
			Content string `json:"content"`
		} `json:"delta"`
	} `json:"choices"`
}

// ExtractLinks Concatenate the relevant citation links.
func (r *ResponsePerplexity) ExtractLinks() string {
	var links string
	for i, citation := range r.Citations {
		links += citation
		if i != len(r.Citations)-1 {
			links += ", "
		} else {
			links += "."
		}
	}
	return links
}

func SearchForQuotes(system, field, authors, promptJuris, prompt string) (*Perplexity, error) {
	log.Println("Estudando a doutrina")
	searchInput := "O ramo do direito é:\n" + field +
		"\nOs doutrinadores relevantes são:\n" + authors +
		"\nO resumo do prompt do usuário é\n" + promptJuris +
		"\nO prompt original do usuário é:\n" + prompt

	papers, err := Search(system, searchInput, "sonar")
	if err != nil {
		return nil, err
	}

	return &Perplexity{
		Response: papers.Choices[0].Message.Content,
		Links:    papers.ExtractLinks(),
	}, nil
}

func SearchForLaws(system, field, prompt, promptJuris string) (*Perplexity, error) {
	log.Println("Estudando as leis relevantes")
	lawInput := "O ramo do direito é:\n" + field +
		"\nO prompt original do usuário é:\n" + prompt +
		"\nO resumo do prompt do usuário é\n" + promptJuris

	laws, err := Search(system, "quais as leis relevantes no site do planalto.gov que se relacionam com: "+lawInput+". Por favor liste todos os links do site do planalto com as leis mencionadas", "sonar")
	if err != nil {
		return nil, err
	}

	return &Perplexity{
		Response: laws.Choices[0].Message.Content,
		Links:    laws.ExtractLinks(),
	}, nil
}

package juit

import (
	"log"
	"sonar/app/gpt"
	"strconv"
	"time"
)

// JurisprudenceResponse represents the structure of the API response.
type JurisprudenceResponse struct {
	Total         int                 `json:"total"`
	Size          int                 `json:"size"`
	NextPageToken string              `json:"next_page_token"`
	SearchInfo    SearchInfo          `json:"search_info"`
	Items         []JurisprudenceItem `json:"items"`
}

// SearchInfo holds metadata about the search.
type SearchInfo struct {
	SearchID        string `json:"search_id"`
	ElapsedTimeInMs int    `json:"elapsed_time_in_ms"`
}

// JurisprudenceItem represents a single jurisprudence record.
type JurisprudenceItem struct {
	ID                   string   `json:"id"`
	JuitID               string   `json:"juit_id"`
	Title                string   `json:"title"`
	Headnote             string   `json:"headnote"`
	FullText             *string  `json:"full_text"` // Nullable field.
	CnjUniqueNumber      string   `json:"cnj_unique_number"`
	OrderDate            string   `json:"order_date"`
	JudgmentDate         string   `json:"judgment_date"`
	PublicationDate      string   `json:"publication_date"`
	ReleaseDate          *string  `json:"release_date"`   // Nullable.
	SignatureDate        *string  `json:"signature_date"` // Nullable.
	CourtCode            string   `json:"court_code"`
	Degree               string   `json:"degree"`
	ProcessOriginState   *string  `json:"process_origin_state"` // Nullable.
	District             string   `json:"district"`
	DocumentMatterList   []string `json:"document_matter_list"`
	ProcessClassNameList []string `json:"process_class_name_list"`
	JudgmentBody         string   `json:"judgment_body"`
	Trier                string   `json:"trier"`
	DocumentType         string   `json:"document_type"`
	JusticeType          string   `json:"justice_type"`
	RimorURL             string   `json:"rimor_url"`
}

// formatDate converts a timestamp string (RFC3339) into "DD/MM/YYYY" format.
func formatDate(dateStr string) string {
	t, err := time.Parse(time.RFC3339, dateStr)
	if err != nil {
		return "" // Handle the error appropriately (e.g., logging)
	}
	return t.Format("02/01/2006") // DD/MM/YYYY format
}

type Jurisprudence struct {
	DocumentType     string
	Degree           string
	ClassSubject     string
	Judge            string
	JudgmentBody     string
	PublicationDate  string
	JudgmentDate     string
	Headnote         string
	FullText         string
	LawsuitReference string
}

func (j *JurisprudenceResponse) GetJurisprudence() []Jurisprudence {
	var r []Jurisprudence

	for _, i := range j.Items {
		var classSubject string
		for k, cS := range i.ProcessClassNameList {
			classSubject += cS
			if k != len(i.ProcessClassNameList)-1 {
				classSubject += " / "
			}
		}

		// Handle nil FullText safely
		fullText := ""
		if i.FullText != nil {
			fullText = *i.FullText
		}

		r = append(r, Jurisprudence{
			DocumentType:     i.DocumentType,
			Degree:           i.Degree,
			ClassSubject:     classSubject,
			Judge:            i.Trier,
			JudgmentBody:     i.JudgmentBody,
			PublicationDate:  formatDate(i.PublicationDate),
			JudgmentDate:     formatDate(i.JudgmentDate),
			Headnote:         i.Headnote,
			FullText:         fullText,
			LawsuitReference: i.CnjUniqueNumber,
		})
	}

	return r
}

func ShouldCallJurisprudencia(system, query string) (bool, error) {
	log.Println("Verificando eligibilidade para jurisprudência")
	relevant, err := gpt.Search(system, query, "gpt-4o-mini")
	if err != nil {
		return false, err
	}

	log.Println("Elegibilidade para jurisprudência: " + relevant.Choices[0].Message.Content)
	if relevant.Choices[0].Message.Content == "Sim" {
		return true, nil
	} else {
		return false, nil
	}
}

func CreateQueryForJurisprudencia(system, query string) (*string, error) {
	log.Println("Criando o prompt ideal para pesquisar a jurisprudência")

	responseKeyWords, err := gpt.Search(system, "transforme esse prompt: "+query+" em uma excelente query para API.", "gpt-4o-mini")
	if err != nil {
		return nil, err
	}

	log.Println("Prompt para jurisprudência: " + responseKeyWords.Choices[0].Message.Content)
	return &responseKeyWords.Choices[0].Message.Content, nil
}

func CallAPIjurisprudencia(query string) (*string, error) {
	log.Println("Consultando jurisprudências")

	jt, err := Search(query)
	if err != nil {
		return nil, err
	}

	jurisInit := jt.GetJurisprudence()
	log.Println("Jurisprudências encontradas: " + strconv.Itoa(len(jurisInit)))

	r := ReturnAsText(jurisInit)
	return r, nil
}

package gpt

import (
	"bytes"
	"encoding/json"
	"errors"
	"github.com/joho/godotenv"
	"io"
	"net/http"
	"os"
)

func Search(system, question, model string) (*OpenAIResponse, error) {
	url := "https://api.openai.com/v1/chat/completions"

	// Struct to properly format JSON payload
	if system == "" {
		system = "Be precise and concise."
	}
	if model != "" {

	}

	// Create request payload
	payload := NewGPTPayload()
	if model != "" {
		payload.Model = model
	}
	payload.NewMessage(Message{
		Role:    "developer",
		Content: system,
	})
	payload.NewMessage(Message{
		Role:    "user",
		Content: question,
	})

	jsonData, err := json.Marshal(payload)
	if err != nil {
		return nil, errors.New("error marshalling JSON: " + err.Error())
	}

	// Create HTTP request
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, errors.New("error creating request: " + err.Error())
	}

	// Load .env file
	err = godotenv.Load(".env")
	if err != nil {
		return nil, errors.New("error loading .env file: " + err.Error())
	}

	token := os.Getenv("OPENAI_API_KEY")
	if token == "" {
		return nil, errors.New("missing OPENAI_API_KEY in environment")
	}

	// Set Headers
	req.Header.Add("Authorization", "Bearer "+token)
	req.Header.Add("Content-Type", "application/json")

	// Send request
	client := &http.Client{}
	res, err := client.Do(req)
	if err != nil {
		return nil, errors.New("error making request: " + err.Error())
	}
	defer res.Body.Close()

	// Read response
	body, err := io.ReadAll(res.Body)
	if err != nil {
		return nil, errors.New("error reading response: " + err.Error())
	}

	if res.StatusCode != http.StatusOK {
		return nil, errors.New("error on response, current status: " + res.Status)
	}

	var result OpenAIResponse
	err = json.Unmarshal(body, &result)
	if err != nil {
		return nil, err
	}

	return &result, nil
}

package gpt

import "log"

// OpenAIRequest represents the payload sent to the OpenAI API for chat completions.
type OpenAIRequest struct {
	// Model specifies the identifier of the model to use for generating the completion.
	// Example: "gpt-3.5-turbo".
	Model string `json:"model"`

	// Messages is an array of message objects that constitute the conversation history.
	// Each message should include a role ("system", "user", or "assistant") and content.
	Messages []Message `json:"messages"`
}

func NewGPTPayload() *OpenAIRequest {
	return &OpenAIRequest{
		Model: "chatgpt-4o-latest",
	}
}

func (o *OpenAIRequest) NewMessage(message Message) {
	o.Messages = append(o.Messages, message)
}

// Message represents a single message in the conversation.
type Message struct {
	// Role specifies the role of the message author.
	// Acceptable values are "system", "user", or "assistant".
	Role string `json:"role"`

	// Content contains the text content of the message.
	Content string `json:"content"`
}

// OpenAIResponse represents the response received from the OpenAI API.
type OpenAIResponse struct {
	// ID is a unique identifier assigned to the completion response.
	ID string `json:"id"`

	// Object specifies the type of object returned. For chat completions, this is typically "chat.completion".
	Object string `json:"object"`

	// Created is a Unix timestamp (in seconds) indicating when the completion was generated.
	Created int64 `json:"created"`

	// Model indicates the identifier of the model used to generate the response.
	Model string `json:"model"`

	// Choices is an array of completion choices returned by the API.
	Choices []Choice `json:"choices"`

	// Usage provides information about token usage for the request and response.
	Usage UsageInfo `json:"usage"`
}

// Choice represents a single completion choice from the API.
type Choice struct {
	// Index indicates the position of this completion choice in the list.
	Index int `json:"index"`

	// Message contains the message generated by the model for this choice.
	Message Message `json:"message"`

	// FinishReason describes the reason why the model stopped generating tokens.
	// Possible values include "stop", "length", "content_filter", or "null" (if the reason is unknown).
	FinishReason string `json:"finish_reason"`
}

// UsageInfo provides information about token usage for the request and response.
type UsageInfo struct {
	// PromptTokens is the number of tokens consumed by the input prompt.
	PromptTokens int `json:"prompt_tokens"`

	// CompletionTokens is the number of tokens generated in the completion.
	CompletionTokens int `json:"completion_tokens"`

	// TotalTokens is the total number of tokens used (prompt + completion).
	TotalTokens int `json:"total_tokens"`
}

func ClassifyLawField(system, query string) (*string, error) {
	log.Println("Classificando o campo do Direito")
	lawField, err := Search(system, query, "gpt-4o-mini")
	if err != nil {
		return nil, err
	}
	log.Println("Campo do Direito: " + lawField.Choices[0].Message.Content)
	return &lawField.Choices[0].Message.Content, err
}

func GetRelevantAuthors(system, query string) (*string, error) {
	log.Println("Selecionando Doutrinadores")
	authors, err := Search(system, query, "gpt-4o-mini")
	if err != nil {
		return nil, err
	}
	log.Println("Doutrinadores selecionados: " + authors.Choices[0].Message.Content)
	return &authors.Choices[0].Message.Content, nil
}

package deepseek

import (
	"bytes"
	"encoding/json"
	"errors"
	"github.com/joho/godotenv"
	"io"
	"net/http"
	"os"
)

func Search(system, question, model string) (*ResponseDeepSeek, error) {
	// DeepSeek API endpoint (compatible with OpenAI's API format)
	url := "https://api.deepseek.com/chat/completions"

	// Provide a default system prompt if not set.
	if system == "" {
		system = "Be precise and concise."
	}

	payload := NewDeepseekPayload()
	if model != "" {
		payload.Model = model
	} else {
		payload.Model = "deepseek-chat"
	}
	payload.NewMessage(Message{
		Role:    "system",
		Content: system,
	})
	payload.NewMessage(Message{
		Role:    "user",
		Content: question,
	})

	// Marshal the payload to JSON.
	jsonData, err := json.Marshal(payload)
	if err != nil {
		return nil, errors.New("error marshalling JSON: " + err.Error())
	}

	// Create the HTTP request.
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, errors.New("error creating request: " + err.Error())
	}

	// Load .env file to fetch environment variables.
	err = godotenv.Load(".env")
	if err != nil {
		return nil, errors.New("error loading .env file: " + err.Error())
	}

	// Retrieve the DeepSeek API key from the environment.
	token := os.Getenv("DEEPSEEK_API_KEY")
	if token == "" {
		return nil, errors.New("missing DEEPSEEK_API_KEY in environment")
	}

	// Set the required headers.
	req.Header.Add("Authorization", "Bearer "+token)
	req.Header.Add("Content-Type", "application/json")

	// Send the HTTP request.
	client := &http.Client{}
	res, err := client.Do(req)
	if err != nil {
		return nil, errors.New("error making request: " + err.Error())
	}
	defer res.Body.Close()

	// Read the response body.
	body, err := io.ReadAll(res.Body)
	if err != nil {
		return nil, errors.New("error reading response: " + err.Error())
	}

	if res.StatusCode != http.StatusOK {
		return nil, errors.New("error on response, current status: " + res.Status)
	}

	// Unmarshal the JSON response into the OpenAIResponse struct.
	var result ResponseDeepSeek
	err = json.Unmarshal(body, &result)
	if err != nil {
		return nil, err
	}

	return &result, nil
}

package deepseek

// AIRequest represents the payload sent to the Deepseek API for chat completions.
type AIRequest struct {
	// Model specifies the identifier of the model to use for generating the completion.
	// Example: "deepseek-chat" or "deepseek-reasoner"
	Model string `json:"model"`

	// Messages is an array of message objects that constitute the conversation history.
	// Each message should include a role ("system", "user", or "assistant") and content.
	Messages []Message `json:"messages"`

	// Stream specifies if the response is streamed or not.
	// The default is false
	Stream bool `json:"stream"`
}

type ResponseDeepSeek struct {
	Id      string `json:"id"`
	Object  string `json:"object"`
	Created int    `json:"created"`
	Model   string `json:"model"`
	Choices []struct {
		Index   int `json:"index"`
		Message struct {
			Role             string `json:"role"`
			Content          string `json:"content"`
			ReasoningContent string `json:"reasoning_content"`
		} `json:"message"`
		Logprobs     interface{} `json:"logprobs"`
		FinishReason string      `json:"finish_reason"`
	} `json:"choices"`
	Usage struct {
		PromptTokens        int `json:"prompt_tokens"`
		CompletionTokens    int `json:"completion_tokens"`
		TotalTokens         int `json:"total_tokens"`
		PromptTokensDetails struct {
			CachedTokens int `json:"cached_tokens"`
		} `json:"prompt_tokens_details"`
		CompletionTokensDetails struct {
			ReasoningTokens int `json:"reasoning_tokens"`
		} `json:"completion_tokens_details"`
		PromptCacheHitTokens  int `json:"prompt_cache_hit_tokens"`
		PromptCacheMissTokens int `json:"prompt_cache_miss_tokens"`
	} `json:"usage"`
	SystemFingerprint string `json:"system_fingerprint"`
}

// Message represents a single message in the conversation.
type Message struct {
	// Role specifies the role of the message author.
	// Acceptable values are "system", "user", or "assistant".
	Role string `json:"role"`

	// Content contains the text content of the message.
	Content string `json:"content"`
}

func NewDeepseekPayload() *AIRequest {
	return &AIRequest{
		Model:  "deepseek-chat",
		Stream: false,
	}
}

func (d *AIRequest) NewMessage(message Message) {
	d.Messages = append(d.Messages, message)
}

//my .env file:
gptRelevantCaseLaw = "Você é um assistente jurídico especializado em classificar perguntas. Sua tarefa é analisar o conteúdo e determinar se ele é relevante para pesquisa de jurisprudência. Responda apenas com 'Sim' ou 'Não'."

gptSimplePrompt = "Você é um assistente jurídico especializado em transformar descrições de casos em consultas de palavras-chave para pesquisa em uma API. Extraia as palavras-chave principais da seguinte descrição e forneça-as separadas por ' + ', sem formatação adicional."

gptClassifier = "Você é um assistente jurídico especializado em classificar textos conforme as áreas do Direito brasileiro. Ao receber um prompt, analise-o e determine a qual ramo do Direito ele pertence, como, por exemplo, Direito Civil, Direito Penal, Direito Trabalhista, Direito Tributário, entre outros. Responda apenas com o nome da área correspondente. Caso não seja possível classificar o tema no Direito Brasileiro, retorne 'Impossível classificar'."

gptAuthors = "Você é um assistente jurídico especializado em doutrina jurídica. Ao receber o nome de um campo do direito (por exemplo, 'Direito Civil', 'Direito Penal', 'Direito Tributário', etc.), identifique e retorne apenas os nomes dos três principais doutrinadores desse campo, listados em ordem de relevância e separados por vírgulas. Se o campo não for reconhecido, responda 'Campo de direito não reconhecido'."

perplexitySearcher = "Você é um assistente de pesquisa jurídica especializado em localizar e referenciar citações de doutrinadores. Ao receber como entrada o nome de um campo do direito, uma lista de doutrinadores relevantes e a pergunta do usuário, sua tarefa é identificar e retornar as citações completas ou pesquise algo como:\"o que o {autor} pensa sobre o {tema}?\" que sejam pertinentes ao ramo indicado e à consulta formulada. Cada citação deve conter o nome do autor, o título da obra e o ano de publicação (quando disponível). Apresente as citações de forma clara, separando-as por ponto e vírgula. Caso o campo não seja reconhecido ou não existam citações relevantes, responda com: 'Citações não encontradas para o campo indicado'."

perplexityLaw = "Você é um assistente jurídico especializado em buscar leis e súmulas relevantes ao prompt passado no site do planalto http://www.planalto.gov.br pois qualquer outro site não é confiável. **Retorne dois itens: 1- o nome da lei e a cópia integral dos artigos relevantes. 2- uma lista de links em formato markdown com o título da lei e o enderec1o URL."

gptSpecialist = "Você é um especialista supremo em todo o direito brasileiro. Com base na pergunta inicial do usuário, nos textos de doutrina fornecidos, nos links incluídos, nas leis enviadas pelo usuário e nas jurisprudências apresentadas, estruture sua resposta nas seguintes seções:\n Forneça sua conclusão final de forma resumida, no máximo um parágrafo\n1. **Explicação:** Forneça uma resposta detalhada, coerente e fundamentada, abordando os principais impactos e argumentos pertinentes.\n2.**Suporte Legislativo:** Apresente o texto fornecido pelo usuário integralmente, sem nehuma modificação. Apresente também todos os links passados pelo usuário que tivem \".gov\", ignore os outros. Mantenha o markdonw.\n3. **Suporte da Doutrina:** Apresente os textos doutrinários completos fornecidos pelo usuário que embasam sua resposta, seguidos de um resumo. **Mantenha a formatação e o conteúdo original dos textos.** \n3.1. **Links Relevantes:** Liste os links de suporte fornecidos. **Preserve os URLs exatamente como foram recebidos.**\n5. **Jurisprudências Relevantes:** Liste as jurisprudências de suporte fornecidas pelo usuário, mantendo o markdown e a formatação original. **Não altere o texto ou a formatação; apenas replique conforme recebido.**.Analise todas as informações disponíveis (exceto as jurisprudências) e, se houver ambiguidades ou divergências entre as fontes, apresente as diferentes interpretações, esclarecendo os fatores que podem influenciar a conclusão final."




